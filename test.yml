version: "3.9"

networks:
  kafka_overlay_net:
    driver: overlay
    attachable: true

configs:
  kafka-connect-jaas.conf:
    file: ./configs/kafka-connect-jaas.conf
  kraft-controller.properties:
    file: ./configs/kraft-controller.properties
  kafka-configs-cli.properties:
    file: ./configs/kafka-configs-cli.properties
  kafka-acls-admin-cli.properties:
    file: ./configs/kafka-acls-admin-cli.properties
  akhq-application.yml:
    file: ./configs/akhq/application.yml

secrets:
  kafka-controller-0-keystore.jks:
    file: ./certs/kafka-controller-0-keystore.jks
  kafka-broker-0-keystore.jks:
    file: ./certs/kafka-broker-0-keystore.jks
  kafka-connect-keystore.jks:
    file: ./certs/kafka-connect-keystore.jks
  truststore.jks:
    file: ./certs/truststore.jks
  connect-rest-credentials.properties: # Keep if Connect REST has basic auth, otherwise remove
    file: ./secrets/connect-rest-credentials.properties
  akhq-credentials.properties: # Keep if AKHQ has basic auth, otherwise remove
    file: ./secrets/akhq-credentials.properties
  ssl-passwords.properties:
    file: ./secrets/ssl-passwords.properties
  kafka-akhq-keystore.jks:
    file: ./certs/kafka-akhq-keystore.jks
  client.properties:
    file: ./client.properties # This is for client-side configuration, not used in the stack but useful for testing

volumes:
  kafka-controller-data:
    external: true
  kafka-broker-data:
    external: true
  kafka-connect-data:
    external: true
  kafka-akhq-data:
    external: true

services:
  mongodb:
    image: mongo
    command: --replSet rs0 --bind_ip_all
    ports:
      - "27017:27017"
    networks:
      - kafka_overlay_net
    deploy:
      restart_policy:
        condition: on-failure

  mongosetup:
    image: mongo
    depends_on:
      - mongodb
    command: >
      mongosh --host mongodb:27017 --eval "rs.initiate({ _id: 'rs0', members: [{ _id: 1, host: 'mongodb:27017' }] })"
    networks:
      - kafka_overlay_net
    deploy:
      restart_policy:
        condition: none

  kafka-controller-0:
    image: confluentinc/cp-kafka:7.9.0
    hostname: kafka-controller-0
    ports:
      - "9093:9093"
    environment:
      CLUSTER_ID: Rv_mOiSXQMSkcOpL_jZ01Q
      KAFKA_PROCESS_ROLES: controller
      KAFKA_NODE_ID: 1001
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1001@kafka-controller-0:9093
      KAFKA_LISTENERS: CONTROLLER://kafka-controller-0:9093
      KAFKA_AUTHORIZER_CLASS_NAME: "org.apache.kafka.metadata.authorizer.StandardAuthorizer"
      KAFKA_SUPER_USERS: "User:CN=kafka-controller-0,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-broker-0,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-connect,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-akhq,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:SSL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      KAFKA_SSL_CLIENT_AUTH: required
      KAFKA_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka-controller-0-keystore.jks
      KAFKA_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/truststore.jks
      KAFKA_SSL_KEYSTORE_PASSWORD: "confluent" # Consider making this a secret
      KAFKA_SSL_TRUSTSTORE_PASSWORD: "confluent" # Consider making this a secret
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

    configs:
      - source: kraft-controller.properties
        target: /etc/kafka/kraft-server.properties
      - source: kafka-configs-cli.properties
        target: /etc/kafka/kafka-configs-cli.properties
      - source: kafka-acls-admin-cli.properties
        target: /etc/kafka/kafka-acls-admin-cli.properties
    secrets:
      - source: kafka-controller-0-keystore.jks
        target: /etc/kafka/secrets/kafka-controller-0-keystore.jks
      - source: truststore.jks
        target: /etc/kafka/secrets/truststore.jks
      - source: kafka-connect-keystore.jks
        target: /etc/kafka/secrets/kafka-connect-keystore.jks
      - source: client.properties
        target: /etc/kafka/secrets/client.properties # This is for client-side configuration, not used in the stack but useful for testing
    volumes:
      - kafka-controller-data:/var/lib/kafka/data
    networks:
      - kafka_overlay_net

    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-broker-0:
    image: confluentinc/cp-kafka:7.9.0
    hostname: kafka-broker-0
    depends_on:
      - kafka-controller-0
    ports:
      - "9092:9092"
    environment:
      CLUSTER_ID: Rv_mOiSXQMSkcOpL_jZ01Q
      KAFKA_PROCESS_ROLES: broker
      KAFKA_NODE_ID: 2001
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1001@kafka-controller-0:9093
      KAFKA_LISTENERS: CLIENT://0.0.0.0:9092,BROKER://0.0.0.0:9091
      KAFKA_ADVERTISED_LISTENERS: CLIENT://kafka-broker-0:9092,BROKER://kafka-broker-0:9091,CONTROLLER://kafka-controller-0:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CLIENT:SSL,BROKER:SSL,CONTROLLER:SSL
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      KAFKA_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka-broker-0-keystore.jks
      KAFKA_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/truststore.jks
      KAFKA_SSL_KEYSTORE_PASSWORD: "confluent"
      KAFKA_SSL_TRUSTSTORE_PASSWORD: "confluent" 
      KAFKA_LISTENER_NAME_BROKER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CONTROLLER_SSL_CLIENT_AUTH: required
      KAFKA_LISTENER_NAME_CLIENT_SSL_CLIENT_AUTH: required 
      KAFKA_AUTHORIZER_CLASS_NAME: "org.apache.kafka.metadata.authorizer.StandardAuthorizer"
      KAFKA_SUPER_USERS: "User:CN=kafka-controller-0,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-broker-0,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-connect,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US;User:CN=kafka-akhq,OU=Confluent,O=TestOrg,L=MountainView,ST=CA,C=US"
      KAFKA_LOG_DIRS: /var/lib/kafka/data/broker-data
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    configs:
      - source: kafka-acls-admin-cli.properties
        target: /etc/kafka/kafka-acls-admin-cli.properties
    secrets:
      - source: kafka-broker-0-keystore.jks
        target: /etc/kafka/secrets/kafka-broker-0-keystore.jks
      - source: truststore.jks
        target: /etc/kafka/secrets/truststore.jks
      - source: kafka-connect-keystore.jks
        target: /etc/kafka/secrets/kafka-connect-keystore.jks
      - source: client.properties
        target: /etc/kafka/secrets/client.properties
    volumes:
      - kafka-broker-data:/var/lib/kafka/data
    networks:
      - kafka_overlay_net
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure

  kafka-connect:
    image: my-kafka-connect-mongodb5
    hostname: kafka-connect
    depends_on:
      - kafka-broker-0
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-broker-0:9092
      CONNECT_GROUP_ID: Rv_mOiSXQMSkcOpL_jZ01Q
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/kafka-connect-jaas.conf"
      
      CONNECT_SECURITY_PROTOCOL: SSL
      CONNECT_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/truststore.jks
      CONNECT_SSL_TRUSTSTORE_PASSWORD: "confluent"
      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka-connect-keystore.jks
      CONNECT_SSL_KEYSTORE_PASSWORD: "confluent"

      CONNECT_CONSUMER_SECURITY_PROTOCOL: SSL
      CONNECT_CONSUMER_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/truststore.jks
      CONNECT_CONSUMER_SSL_TRUSTSTORE_PASSWORD: "confluent"
      CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_CONSUMER_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka-connect-keystore.jks
      CONNECT_CONSUMER_SSL_KEYSTORE_PASSWORD: "confluent"

      CONNECT_PRODUCER_SECURITY_PROTOCOL: SSL
      CONNECT_PRODUCER_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/truststore.jks
      CONNECT_PRODUCER_SSL_TRUSTSTORE_PASSWORD: "confluent"
      CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_PRODUCER_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka-connect-keystore.jks
      CONNECT_PRODUCER_SSL_KEYSTORE_PASSWORD: "confluent"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1

      CONNECT_LISTENERS: https://0.0.0.0:8083
      CONNECT_REST_EXTENSION_CLASSES: org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension
      CONNECT_BASIC_AUTH_CREDENTIALS_SOURCE: FILE
      CONNECT_BASIC_AUTH_FILE: /etc/kafka/secrets/connect-rest-credentials.properties
      CONNECT_PLUGIN_PATH: "/usr/share/confluent-hub-components"
    
    configs:
      - source: kafka-connect-jaas.conf
        target: /etc/kafka/kafka-connect-jaas.conf
    secrets:
      - source: truststore.jks
        target: /etc/kafka/secrets/truststore.jks
      - source: kafka-connect-keystore.jks
        target: /etc/kafka/secrets/kafka-connect-keystore.jks
      - source: connect-rest-credentials.properties # Keep if you want basic auth on Connect REST, otherwise remove
        target: /etc/kafka/secrets/connect-rest-credentials.properties
      - source: ssl-passwords.properties
        target: /etc/kafka/secrets/ssl-passwords.properties
    volumes:
      - kafka-connect-data:/var/lib/kafka/data
    networks:
      - kafka_overlay_net
    healthcheck: 
      test: ["CMD-SHELL", "nc -z localhost 8083 || exit 1"]
      interval: 30s       
      timeout: 10s        
      retries: 3         
      start_period: 60s  
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure

  akhq:
    image: tchiotludo/akhq:latest
    hostname: akhq
    ports:
    - target: 8080
      published: 8080
      protocol: tcp
      mode: host
    depends_on:
      - kafka-broker-0
      - kafka-connect
    environment:
      MICRONAUT_CONFIG_FILES: "/app/application.yml"
      AKHQ_TRUSTSTORE_PASSWORD: "confluent" 
      AKHQ_KEYSTORE_PASSWORD: "confluent"
    configs:
      - source: akhq-application.yml
        target: /app/application.yml
    secrets:
      - source: truststore.jks
        target: /etc/akhq/secrets/truststore.jks
      - source: akhq-credentials.properties # Keep if AKHQ has basic auth, otherwise remove
        target: /etc/akhq/secrets/akhq-credentials.properties
      - source: ssl-passwords.properties
        target: /etc/akhq/secrets/ssl-passwords.properties
      - source: kafka-akhq-keystore.jks
        target: /etc/akhq/secrets/kafka-akhq-keystore.jks
    volumes:
      - kafka-akhq-data:/var/lib/akhq/data
    networks:
      - kafka_overlay_net
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure